{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "410c1743-9fa0-4b2a-b503-6afe70711041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Spark Session\n",
      "3.1.1-amzn-0\n",
      "test constraint check set 1\n",
      "Reading s3://tahoeqa-interim-data/nvd/nvd Data\n",
      "PythonCallback server restarted!\n",
      "[Row(constraint_status='Success'), Row(constraint_status='Success'), Row(constraint_status='Success'), Row(constraint_status='Success'), Row(constraint_status='Success')]\n",
      "Passed - \tCompletenessConstraint(Completeness(publishedDate,None)) passed\n",
      "Passed - \tComplianceConstraint(Compliance(publishedDate is non-negative,COALESCE(CAST(publishedDate AS DECIMAL(20,10)), 0.0) >= 0,None)) passed\n",
      "Passed - \tCompletenessConstraint(Completeness(lastModifiedDate,None)) passed\n",
      "Passed - \tComplianceConstraint(Compliance(lastModifiedDate is non-negative,COALESCE(CAST(lastModifiedDate AS DECIMAL(20,10)), 0.0) >= 0,None)) passed\n",
      "Passed - \tSizeConstraint(Size(None)) passed\n",
      "*********************************************************\n",
      "test constraint check set 2\n",
      "Reading s3://tahoeqa-interim-data/nvd/vendor Data\n",
      "[Row(constraint_status='Success')]\n",
      "Passed - \tSizeConstraint(Size(None)) passed\n",
      "*********************************************************\n",
      "test constraint check set 3\n",
      "Reading s3://tahoeqa-interim-data/nvd/nvd_configurations.nodes.val.cpe_match.val.cpe_name Data\n",
      "[Row(constraint_status='Success')]\n",
      "Passed - \tSizeConstraint(Size(None)) passed\n",
      "*********************************************************\n",
      "test constraint check set 4\n",
      "Reading s3://tahoeqa-interim-data/nvd/nvd_configurations.nodes.val.cpe_match Data\n",
      "[Row(constraint_status='Success')]\n",
      "Passed - \tSizeConstraint(Size(None)) passed\n",
      "*********************************************************\n",
      "test constraint check set 5\n",
      "Reading s3://tahoeqa-interim-data/nvd/nvd_cve.description.description_data Data\n",
      "[Row(constraint_status='Success')]\n",
      "Passed - \tSizeConstraint(Size(None)) passed\n",
      "*********************************************************\n",
      "test constraint check set 6\n",
      "Reading s3://tahoeqa-interim-data/nvd/nvd_cve.problemtype.problemtype_data.val.description Data\n",
      "[Row(constraint_status='Success')]\n",
      "Passed - \tSizeConstraint(Size(None)) passed\n",
      "*********************************************************\n",
      "test constraint check set 7\n",
      "Reading s3://tahoeqa-interim-data/nvd/nvd_cve.problemtype.problemtype_data Data\n",
      "[Row(constraint_status='Success')]\n",
      "Passed - \tSizeConstraint(Size(None)) passed\n",
      "*********************************************************\n",
      "test constraint check set 8\n",
      "Reading s3://tahoeqa-interim-data/nvd/nvd_cve.references.reference_data.val.tags Data\n",
      "[Row(constraint_status='Success')]\n",
      "Passed - \tSizeConstraint(Size(None)) passed\n",
      "*********************************************************\n",
      "test constraint check set 9\n",
      "Reading s3://tahoeqa-interim-data/nvd/nvd_cve.references.reference_data Data\n",
      "[Row(constraint_status='Success')]\n",
      "Passed - \tSizeConstraint(Size(None)) passed\n",
      "*********************************************************\n",
      "test constraint check set 10\n",
      "Reading s3://tahoeqa-interim-data/nvd/nvd_configurations.nodes.val.children Data\n",
      "[Row(constraint_status='Success')]\n",
      "Passed - \tSizeConstraint(Size(None)) passed\n",
      "*********************************************************\n",
      "test constraint check set 11\n",
      "Reading s3://tahoeqa-interim-data/nvd/nvd_configurations.nodes.val.children.val.cpe_match Data\n",
      "[Row(constraint_status='Success')]\n",
      "Passed - \tSizeConstraint(Size(None)) passed\n",
      "*********************************************************\n",
      "test constraint check set 12\n",
      "Reading s3://tahoeqa-interim-data/nvd/nvd_configurations.nodes.val.children.val.cpe_match.val.cpe_name Data\n",
      "[Row(constraint_status='Success')]\n",
      "Passed - \tSizeConstraint(Size(None)) passed\n",
      "*********************************************************\n",
      "test constraint check set 13\n",
      "Reading s3://tahoeqa-interim-data/nvd/nvd_configurations.nodes.val.children.val.children Data\n",
      "[Row(constraint_status='Success')]\n",
      "Passed - \tSizeConstraint(Size(None)) passed\n",
      "*********************************************************\n",
      "*********************************************************\n",
      "Total Passed Tests = 68\n",
      "Total Failed Tests = 0\n",
      "Total Tests = 68\n",
      "*********************************************************\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pydeequ\n",
    "from pydeequ import Check, CheckLevel, AnalysisRunner\n",
    "from pydeequ.analyzers import *\n",
    "from pydeequ.suggestions import *\n",
    "from pydeequ.repository import FileSystemMetricsRepository, ResultKey\n",
    "from pydeequ.verification import VerificationSuite, VerificationResult\n",
    "import time\n",
    "import pytest\n",
    "import import_ipynb\n",
    "from test_methods import *\n",
    "\n",
    "\n",
    "# Set Up\n",
    "spark = create_session()\n",
    "print(spark.version)\n",
    "\n",
    "\n",
    "# Analysis tools\n",
    "# generate_suggestions(parquet_files)\n",
    "# analyze_file(parquet_files)\n",
    "# check_constraints(parquet_files)\n",
    "\n",
    "\n",
    "# Tests\n",
    "check = Check(spark, CheckLevel.Warning, \"NVD Intrim Parquet Data Check\")\n",
    "# tc 1 *********************************************************\n",
    "print(\"test constraint check set 1\")\n",
    "print(\"Reading \" + 's3://tahoeqa-interim-data/nvd/nvd' + \" Data\")\n",
    "df = spark.read.parquet('s3://tahoeqa-interim-data/nvd/nvd')\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addCheck(\n",
    "        check.isComplete(\"publishedDate\")  \\\n",
    "        #.isUnique(\"publishedDate\")  \\\n",
    "        .isNonNegative(\"publishedDate\") \\\n",
    "        .isComplete(\"lastModifiedDate\")  \\\n",
    "        #.isUnique(\"lastModifiedDate\")  \\\n",
    "        .isNonNegative(\"lastModifiedDate\") \\\n",
    "        .hasSize(lambda x: x >= 100000)) \\\n",
    "    .run()\n",
    "\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "#checkResult_df.show()\n",
    "print(checkResult_df.select(\"constraint_status\").collect())\n",
    "\n",
    "check_results(checkResult.checkResults)\n",
    "print(\"*********************************************************\")\n",
    "\n",
    "\n",
    "\n",
    "check = Check(spark, CheckLevel.Warning, \"NVD Intrim Parquet Data Check\")\n",
    "# tc 2 *********************************************************\n",
    "print(\"test constraint check set 2\")\n",
    "print(\"Reading \" + 's3://tahoeqa-interim-data/nvd/vendor' + \" Data\")\n",
    "df = spark.read.parquet('s3://tahoeqa-interim-data/nvd/vendor')\n",
    "# time.sleep(10)\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addCheck(\n",
    "        check.hasSize(lambda x: x >= 100000)) \\\n",
    "    .run()\n",
    "# time.sleep(10)\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "#checkResult_df.show()\n",
    "print(checkResult_df.select(\"constraint_status\").collect())\n",
    "check_results(checkResult.checkResults)\n",
    "print(\"*********************************************************\")\n",
    "\n",
    "\n",
    "check = Check(spark, CheckLevel.Warning, \"NVD Intrim Parquet Data Check\")\n",
    "# tc 3 *********************************************************\n",
    "print(\"test constraint check set 3\")\n",
    "print(\"Reading \" + 's3://tahoeqa-interim-data/nvd/nvd_configurations.nodes.val.cpe_match.val.cpe_name' + \" Data\")\n",
    "df = spark.read.parquet('s3://tahoeqa-interim-data/nvd/nvd_configurations.nodes.val.cpe_match.val.cpe_name')\n",
    "# time.sleep(10)\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addCheck(\n",
    "        check.hasSize(lambda x: x >= 100000)) \\\n",
    "    .run()\n",
    "# time.sleep(10)\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "#checkResult_df.show()\n",
    "print(checkResult_df.select(\"constraint_status\").collect())\n",
    "check_results(checkResult.checkResults)\n",
    "print(\"*********************************************************\")\n",
    "\n",
    "\n",
    "check = Check(spark, CheckLevel.Warning, \"NVD Intrim Parquet Data Check\")\n",
    "# tc 4 *********************************************************\n",
    "print(\"test constraint check set 4\")\n",
    "print(\"Reading \" + 's3://tahoeqa-interim-data/nvd/nvd_configurations.nodes.val.cpe_match' + \" Data\")\n",
    "df = spark.read.parquet('s3://tahoeqa-interim-data/nvd/nvd_configurations.nodes.val.cpe_match')\n",
    "# time.sleep(10)\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addCheck(\n",
    "        check.hasSize(lambda x: x >= 100000)) \\\n",
    "    .run()\n",
    "# time.sleep(10)\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "#checkResult_df.show()\n",
    "print(checkResult_df.select(\"constraint_status\").collect())\n",
    "check_results(checkResult.checkResults)\n",
    "print(\"*********************************************************\")\n",
    "\n",
    "\n",
    "check = Check(spark, CheckLevel.Warning, \"NVD Intrim Parquet Data Check\")\n",
    "# tc 5 *********************************************************\n",
    "print(\"test constraint check set 5\")\n",
    "print(\"Reading \" + 's3://tahoeqa-interim-data/nvd/nvd_cve.description.description_data' + \" Data\")\n",
    "df = spark.read.parquet('s3://tahoeqa-interim-data/nvd/nvd_cve.description.description_data')\n",
    "# time.sleep(10)\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addCheck(\n",
    "        check.hasSize(lambda x: x >= 100000)) \\\n",
    "    .run()\n",
    "# time.sleep(10)\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "#checkResult_df.show()\n",
    "print(checkResult_df.select(\"constraint_status\").collect())\n",
    "check_results(checkResult.checkResults)\n",
    "print(\"*********************************************************\")\n",
    "\n",
    "\n",
    "check = Check(spark, CheckLevel.Warning, \"NVD Intrim Parquet Data Check\")\n",
    "# tc 6 *********************************************************\n",
    "print(\"test constraint check set 6\")\n",
    "print(\"Reading \" + 's3://tahoeqa-interim-data/nvd/nvd_cve.problemtype.problemtype_data.val.description' + \" Data\")\n",
    "df = spark.read.parquet('s3://tahoeqa-interim-data/nvd/nvd_cve.problemtype.problemtype_data.val.description')\n",
    "# time.sleep(10)\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addCheck(\n",
    "        check.hasSize(lambda x: x >= 100000)) \\\n",
    "    .run()\n",
    "# time.sleep(10)\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "#checkResult_df.show()\n",
    "print(checkResult_df.select(\"constraint_status\").collect())\n",
    "check_results(checkResult.checkResults)\n",
    "print(\"*********************************************************\")\n",
    "\n",
    "\n",
    "check = Check(spark, CheckLevel.Warning, \"NVD Intrim Parquet Data Check\")\n",
    "# tc 7 *********************************************************\n",
    "print(\"test constraint check set 7\")\n",
    "print(\"Reading \" + 's3://tahoeqa-interim-data/nvd/nvd_cve.problemtype.problemtype_data' + \" Data\")\n",
    "df = spark.read.parquet('s3://tahoeqa-interim-data/nvd/nvd_cve.problemtype.problemtype_data')\n",
    "# time.sleep(10)\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addCheck(\n",
    "        check.hasSize(lambda x: x >= 100000)) \\\n",
    "    .run()\n",
    "# time.sleep(10)\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "#checkResult_df.show()\n",
    "print(checkResult_df.select(\"constraint_status\").collect())\n",
    "check_results(checkResult.checkResults)\n",
    "print(\"*********************************************************\")\n",
    "\n",
    "\n",
    "check = Check(spark, CheckLevel.Warning, \"NVD Intrim Parquet Data Check\")\n",
    "# tc 8 *********************************************************\n",
    "print(\"test constraint check set 8\")\n",
    "print(\"Reading \" + 's3://tahoeqa-interim-data/nvd/nvd_cve.references.reference_data.val.tags' + \" Data\")\n",
    "df = spark.read.parquet('s3://tahoeqa-interim-data/nvd/nvd_cve.references.reference_data.val.tags')\n",
    "# time.sleep(10)\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addCheck(\n",
    "        check.hasSize(lambda x: x >= 100000)) \\\n",
    "    .run()\n",
    "# time.sleep(10)\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "#checkResult_df.show()\n",
    "print(checkResult_df.select(\"constraint_status\").collect())\n",
    "check_results(checkResult.checkResults)\n",
    "print(\"*********************************************************\")\n",
    "\n",
    "\n",
    "check = Check(spark, CheckLevel.Warning, \"NVD Intrim Parquet Data Check\")\n",
    "# tc 9 *********************************************************\n",
    "print(\"test constraint check set 9\")\n",
    "print(\"Reading \" + 's3://tahoeqa-interim-data/nvd/nvd_cve.references.reference_data' + \" Data\")\n",
    "df = spark.read.parquet('s3://tahoeqa-interim-data/nvd/nvd_cve.references.reference_data')\n",
    "# time.sleep(10)\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addCheck(\n",
    "        check.hasSize(lambda x: x >= 100000)) \\\n",
    "    .run()\n",
    "# time.sleep(10)\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "#checkResult_df.show()\n",
    "print(checkResult_df.select(\"constraint_status\").collect())\n",
    "check_results(checkResult.checkResults)\n",
    "print(\"*********************************************************\")\n",
    "\n",
    "\n",
    "check = Check(spark, CheckLevel.Warning, \"NVD Intrim Parquet Data Check\")\n",
    "# tc 10 *********************************************************\n",
    "print(\"test constraint check set 10\")\n",
    "print(\"Reading \" + 's3://tahoeqa-interim-data/nvd/nvd_configurations.nodes.val.children' + \" Data\")\n",
    "df = spark.read.parquet('s3://tahoeqa-interim-data/nvd/nvd_configurations.nodes.val.children')\n",
    "# time.sleep(10)\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addCheck(\n",
    "        check.hasSize(lambda x: x >= 100000)) \\\n",
    "    .run()\n",
    "# time.sleep(10)\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "#checkResult_df.show()\n",
    "print(checkResult_df.select(\"constraint_status\").collect())\n",
    "check_results(checkResult.checkResults)\n",
    "print(\"*********************************************************\")\n",
    "\n",
    "\n",
    "check = Check(spark, CheckLevel.Warning, \"NVD Intrim Parquet Data Check\")\n",
    "# tc 11 *********************************************************\n",
    "print(\"test constraint check set 11\")\n",
    "print(\"Reading \" + 's3://tahoeqa-interim-data/nvd/nvd_configurations.nodes.val.children.val.cpe_match' + \" Data\")\n",
    "df = spark.read.parquet('s3://tahoeqa-interim-data/nvd/nvd_configurations.nodes.val.children.val.cpe_match')\n",
    "# time.sleep(10)\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addCheck(\n",
    "        check.hasSize(lambda x: x >= 100000)) \\\n",
    "    .run()\n",
    "# time.sleep(10)\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "#checkResult_df.show()\n",
    "print(checkResult_df.select(\"constraint_status\").collect())\n",
    "check_results(checkResult.checkResults)\n",
    "print(\"*********************************************************\")\n",
    "\n",
    "\n",
    "check = Check(spark, CheckLevel.Warning, \"NVD Intrim Parquet Data Check\")\n",
    "# tc 12 *********************************************************\n",
    "print(\"test constraint check set 12\")\n",
    "print(\"Reading \" + 's3://tahoeqa-interim-data/nvd/nvd_configurations.nodes.val.children.val.cpe_match.val.cpe_name' + \" Data\")\n",
    "df = spark.read.parquet('s3://tahoeqa-interim-data/nvd/nvd_configurations.nodes.val.children.val.cpe_match.val.cpe_name')\n",
    "# time.sleep(10)\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addCheck(\n",
    "        check.hasSize(lambda x: x >= 100000)) \\\n",
    "    .run()\n",
    "# time.sleep(10)\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "#checkResult_df.show()\n",
    "print(checkResult_df.select(\"constraint_status\").collect())\n",
    "check_results(checkResult.checkResults)\n",
    "print(\"*********************************************************\")\n",
    "\n",
    "\n",
    "check = Check(spark, CheckLevel.Warning, \"NVD Intrim Parquet Data Check\")\n",
    "# tc 13 *********************************************************\n",
    "print(\"test constraint check set 13\")\n",
    "print(\"Reading \" + 's3://tahoeqa-interim-data/nvd/nvd_configurations.nodes.val.children.val.children' + \" Data\")\n",
    "df = spark.read.parquet('s3://tahoeqa-interim-data/nvd/nvd_configurations.nodes.val.children.val.children')\n",
    "# time.sleep(10)\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addCheck(\n",
    "        check.hasSize(lambda x: x >= 100000)) \\\n",
    "    .run()\n",
    "# time.sleep(10)\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "#checkResult_df.show()\n",
    "print(checkResult_df.select(\"constraint_status\").collect())\n",
    "check_results(checkResult.checkResults)\n",
    "print(\"*********************************************************\")\n",
    "print(\"*********************************************************\")\n",
    "print(\"Total Passed Tests = \" + str(tpass_count))\n",
    "print(\"Total Failed Tests = \" + str(tfail_count))\n",
    "print(\"Total Tests = \" + str(tpass_count + tfail_count))\n",
    "print(\"*********************************************************\")\n",
    "\n",
    "#Clean up\n",
    "spark.sparkContext._gateway.shutdown_callback_server()\n",
    "spark.stop()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3fd9a8-48ef-47cb-bcda-c86c64dd2495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
